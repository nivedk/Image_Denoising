{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptual Loss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-WSuLgoXf77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOn0Az97gesl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otz47e6OsXO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBXwtm37sEXm",
        "colab_type": "code",
        "outputId": "2e6e8e2f-f060-4465-a59a-5dd6965bb66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install -q -U 'tensorboard<2.2.0,>=2.1.0'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.9MB 6.2MB/s \n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5LcfD_hsxPS",
        "colab_type": "text"
      },
      "source": [
        "Especifica las direcciones donde se encuentran los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfO67voAchG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = \"/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/rain\"\n",
        "folders = ['training','val','test_syn']\n",
        "\n",
        "INPATH = PATH+'/'+folders[0]\n",
        "VALNPATH = PATH+'/'+folders[1]\n",
        "TESTPATH = PATH+'/'+folders[2]\n",
        "CHECKPOINT = \"/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/checkpoint/\"\n",
        "\n",
        "train_urls = !ls -1 \"{INPATH}\"\n",
        "val_urls = !ls -1 \"{VALNPATH}\"\n",
        "test_imgurls = !ls -1 \"{TESTPATH}\"\n",
        "\n",
        "INP = [INPATH+'/'+i for i in train_urls]\n",
        "VALP = [VALNPATH+'/'+i for i in val_urls]\n",
        "TESTP = [TESTPATH+'/'+i for i in test_imgurls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01qBtBbxW0kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvs366JZtBVY",
        "colab_type": "text"
      },
      "source": [
        "Funcion para cargar las imagenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkhFIFgDW6DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(path_to_image):\n",
        "  image = tf.io.read_file(path_to_image)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "\n",
        "  real = image[:,:tf.shape(image)[1]//2,:]\n",
        "  fake = image[:,tf.shape(image)[1]//2:,:]\n",
        "\n",
        "  real = tf.cast(real,tf.float32)\n",
        "  fake = tf.cast(fake,tf.float32)\n",
        "  \n",
        "  return fake, real"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMVAi0yNUXGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_im = '/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/rain/train/1.jpg'\n",
        "fake, real = load(path_to_im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avtfU4V7T-3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,ax = plt.subplots(nrows=1,ncols=2)\n",
        "ax[1].imshow(real/255.0)\n",
        "ax[0].imshow(fake/255.0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcAJuW34UBui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(image_input,image_real,h,w):\n",
        "  image_input = tf.image.resize(image_input,(h,w),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  image_real = tf.image.resize(image_real,(h,w),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  return image_input, image_real"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxuatBugLRhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(fake.shape)\n",
        "fake, hhhhh= resize(fake,fake,256,256)\n",
        "print(fake.shape)\n",
        "print(hhhhh.shape)\n",
        "fig,ax = plt.subplots(nrows=1,ncols=2)\n",
        "ax[1].imshow(hhhhh/255.0)\n",
        "ax[0].imshow(fake/255.0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZhNimobl9Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image],axis=0)\n",
        "  cropped_image = tf.image.random_crop(stacked_image,size=(2,IMG_HEIGHT,IMG_WIDTH,3))\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OHeQHCen5Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(input_image, real_image):\n",
        "  input_image = input_image/(255/2) - 1\n",
        "  real_image = real_image/(255/2) - 1\n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orVCDX2NwimS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image,real_image):\n",
        "  input_image, real_image = resize(input_image, real_image,286,286) # resizing to 286 x 286 x 3\n",
        "  input_image, real_image = random_crop(input_image, real_image) # randomly cropping to 256 x 256 x 3\n",
        "  if np.random.uniform()>0.5: # random mirroring\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    real_image = tf.image.flip_left_right(real_image)\n",
        "  return input_image, real_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7hqz-DiPRlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#real, fake = tf.map_fn(lambda path: load(path),path_to_im,)\n",
        "in_im,re_im = random_jitter(fake, real)\n",
        "fig,ax = plt.subplots(nrows=1,ncols=2)\n",
        "ax[1].imshow(re_im/255.0)\n",
        "ax[0].imshow(in_im/255.0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwJbrTsMLOPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El9qW3H0P02Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train(image_path):\n",
        "  input_image, real_image = load(image_path)\n",
        "  input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "  return input_image, real_image\n",
        "\n",
        "def load_test(image_path):\n",
        "  input_image, real_image = load(image_path)\n",
        "  input_image, real_image = resize(input_image, real_image,IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "  return input_image, real_image\n",
        "\n",
        "def load_validation(image_path):\n",
        "  return load_test(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIEM7DcGQciX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.list_files('/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/rain/train/*.jpg')\n",
        "train_data = train_data.map(load_train,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_data = train_data.shuffle(BATCH_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "vali_data = tf.data.Dataset.list_files('/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/rain/test_syn/*.jpg')\n",
        "vali_data = vali_data.map(load_test)\n",
        "vali_data = vali_data.batch(BATCH_SIZE)\n",
        "\n",
        "test_data = tf.data.Dataset.list_files('/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/rain/val/*.jpg')\n",
        "test_data = test_data.map(load_test)\n",
        "test_data = test_data.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrGsylosZgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def down_sample(filters,size,apply_batchNorm=True):\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(tf.keras.layers.Conv2D(filters,size,strides=2,padding='same',\n",
        "                                    kernel_initializer=tf.random_normal_initializer(0.,0.02),\n",
        "                                    use_bias=False))\n",
        "  if apply_batchNorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6DqlxJiyaD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def up_sample(filters,size,apply_dropOut=False):\n",
        "  result = tf.keras.Sequential()\n",
        "  #result.add(tf.keras.layers.UpSampling2D(size=(2, 2)))\n",
        "  result.add(tf.keras.layers.Conv2DTranspose(filters,size,strides=2,padding='same',\n",
        "                                    kernel_initializer=tf.random_normal_initializer(0.,0.02),\n",
        "                                    use_bias=False))\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "  if apply_dropOut:\n",
        "    result.add(tf.keras.layers.Dropout(0.5))\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki77eL_XzSM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=(256,256,3))\n",
        "\n",
        "  down_stack = [down_sample(64,4,apply_batchNorm=False), # (bs, 128, 128, 64)\n",
        "                down_sample(128,4), # (bs, 64, 64, 128)\n",
        "                down_sample(256,4), # (bs, 32, 32, 256)\n",
        "                down_sample(512,4), # (bs, 16, 16, 512)\n",
        "                down_sample(512,4), # (bs, 8, 8, 512)\n",
        "                down_sample(512,4), # (bs, 4, 4, 512)\n",
        "                down_sample(512,4), # (bs, 2, 2, 512)\n",
        "                down_sample(512,4)] # (bs, 1, 1, 512)\n",
        "\n",
        "  up_stack = [up_sample(512,4,apply_dropOut=True), # (bs, 2, 2, 1024)\n",
        "                up_sample(512,4,apply_dropOut=True), # (bs, 4, 4, 1024)\n",
        "                up_sample(512,4,apply_dropOut=True), # (bs, 8, 8, 1024)\n",
        "                up_sample(512,4), # (bs, 16, 16, 1024)\n",
        "                up_sample(256,4), # (bs, 32, 32, 512)\n",
        "                up_sample(128,4), # (bs, 64, 64, 256)\n",
        "                up_sample(64,4), # (bs, 128, 128, 128)\n",
        "                ]\n",
        "  last = tf.keras.layers.Conv2DTranspose(3,4,strides=2,padding='same',\n",
        "                                           kernel_initializer=tf.random_normal_initializer(0.,0.02),\n",
        "                                           activation='tanh')\n",
        "  \n",
        "  x = inputs\n",
        "  skip_connections = []\n",
        "  for layer in down_stack:\n",
        "    x = layer(x)\n",
        "    skip_connections.append(x)\n",
        "  \n",
        "  skip_connections = reversed(skip_connections[:-1])\n",
        "\n",
        "  for layer, u_net in zip(up_stack,skip_connections):\n",
        "    x = layer(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, u_net])\n",
        "\n",
        "  out = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs,outputs=out)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvKIXea-zrhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator,show_shapes=True,dpi=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxD4yG_oNCx6",
        "colab_type": "code",
        "outputId": "409fbb56-0987-4df1-fdcc-b292a4e21c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(fake.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVsHybAhp6Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_output = generator(fake[tf.newaxis,...], training=False)\n",
        "plt.imshow(gen_output[0,...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jigwUkWqKvLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input,decode_predictions\n",
        "model=VGG16(weights='imagenet',include_top=False)\n",
        "\n",
        "class contentLoss():\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        for layer in model.layers:\n",
        "            layer.trainable=False\n",
        "\n",
        "    def forward(self,reconstructed,reference):\n",
        "        #layer_outputs = [layer.output for layer in model.layers]\n",
        "        reconstructed=tf.keras.backend.expand_dims(reconstructed,axis=-1)\n",
        "        rec_feats=(model(inputs=reconstructed))\n",
        "\n",
        "        reference=tf.keras.backend.expand_dims(reference,axis=-1)\n",
        "        ref_feats=(model(inputs=reference))\n",
        "        mse= tf.keras.losses.MeanSquaredError()\n",
        "        loss_val=mse(ref_feats,rec_feats)\n",
        "        return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIge2ToP4FO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LAMBDA = 100\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  percep_loss=contentLoss().forward(gen_output,target) \n",
        "  #print(type(ref_feats))\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss) + percep_loss\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy2pkSy_5qvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = down_sample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = down_sample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = down_sample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cm1OHF3U1_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fyAJnjDVEmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disc_out = discriminator([fake[tf.newaxis,...], gen_output], training=False)\n",
        "print(type(disc_out))\n",
        "print(tf.shape(disc_out))\n",
        "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qnOX3jsWZ9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLOgVsyPqStI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g80iWflBqS7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJjEoq4FqV9k",
        "colab_type": "code",
        "outputId": "9599b996-47e1-4b78-8ab4-c17cad7cedcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint_prefix = '/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/checkpoint/ckpt'\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=2)\n",
        "\n",
        "checkpoint.restore(ckpt_manager.latest_checkpoint)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  print(\"Restaurado de {}\".format(ckpt_manager.latest_checkpoint))\n",
        "else:\n",
        "  print(\"Inicializando desde cero.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicializando desde cero.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voQ2bL-gqrn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZWbMuhPqwbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for example_input, example_target in test_data.take(1):\n",
        "  plt.imshow(example_input[0])\n",
        "  plt.show()\n",
        "  generate_images(generator, example_input, example_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7RSozmtqyKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100\n",
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcfPvVIWtJ7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYEHRt3YtLgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(train_ds, epochs, test_ds):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    for example_input, example_target in test_ds.take(1):\n",
        "      generate_images(generator, example_input, example_target)\n",
        "    print(\"Epoch: \", epoch)\n",
        "\n",
        "    # Train\n",
        "    for n, (input_image, target) in train_ds.enumerate():\n",
        "      print('.', end='')\n",
        "      if (n+1) % 100 == 0:\n",
        "        print()\n",
        "      train_step(input_image, target, epoch)\n",
        "    print()\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XytdhsBDtOE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir {log_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pnwAxPMtRQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit(train_data, EPOCHS, vali_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjIuE3sZtZ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a!tensorboard dev upload --logdir  {log_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P09yJnQxtqMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrD7wUILtw8Q",
        "colab_type": "code",
        "outputId": "d043da6d-dc1b-4f22-fc9e-d7aa58e33cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(\"/content/drive/My Drive/Colab Notebooks/IDCGAN_Github/Single-Image-De-Raining-Keras/dataset/checkpoint/\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff606d292b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13lWKMoZt3Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the trained model on a few examples from the test dataset\n",
        "for inp, tar in test_data.take(100):\n",
        "  generate_images(generator, inp, tar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOos1SESzF7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}